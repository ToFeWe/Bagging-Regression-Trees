%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Robin Kraft at 2018-02-25 3:24:50 PM +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{Dietterich2000,
	Abstract = {Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a ``base'' learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.},
	Author = {Dietterich, Thomas G.},
	Date-Added = {2018-02-14 12:16:50 PM +0000},
	Date-Modified = {2018-02-14 12:16:50 PM +0000},
	Day = {01},
	Doi = {10.1023/A:1007607513941},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Aug},
	Number = {2},
	Pages = {139--157},
	Title = {An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization},
	Volume = {40},
	Year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1007607513941},
	Bdsk-Url-2 = {http://dx.doi.org/10.1023/A:1007607513941}}

@article{buja2000,
	Author = {Buja, Andreas and Stuetzle, Werner},
	Date-Added = {2018-02-14 12:16:50 PM +0000},
	Date-Modified = {2018-02-14 12:16:50 PM +0000},
	Month = {10},
	Title = {Bagging Does Not Always Decrease Mean Squared Error},
	Year = {2000}}

@article{Grandvalet2004,
	Abstract = {Bagging constructs an estimator by averaging predictors trained on bootstrap samples. Bagged estimates almost consistently improve on the original predictor. It is thus important to understand the reasons for this success, and also for the occasional failures. It is widely believed that bagging is effective thanks to the variance reduction stemming from averaging predictors. However, seven years from its introduction, bagging is still not fully understood. This paper provides experimental evidence supporting the hypothesis that bagging stabilizes prediction by equalizing the influence of training examples. This effect is detailed in two different frameworks: estimation on the real line and regression. Bagging's improvements/deteriorations are explained by the goodness/badness of highly influential examples, in situations where the usual variance reduction argument is at best questionable. Finally, reasons for the equalization effect are advanced. They support that other resampling strategies such as half-sampling should provide qualitatively identical effects while being computationally less demanding than bootstrap sampling.},
	Author = {Grandvalet, Yves},
	Date-Added = {2018-02-14 12:16:50 PM +0000},
	Date-Modified = {2018-02-14 12:16:50 PM +0000},
	Day = {01},
	Doi = {10.1023/B:MACH.0000027783.34431.42},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Jun},
	Number = {3},
	Pages = {251--270},
	Title = {Bagging Equalizes Influence},
	Volume = {55},
	Year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1023/B:MACH.0000027783.34431.42},
	Bdsk-Url-2 = {http://dx.doi.org/10.1023/B:MACH.0000027783.34431.42}}

@article{Bauer1999,
	Abstract = {Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only ``hard'' areas but also outliers and noise.},
	Author = {Bauer, Eric and Kohavi, Ron},
	Date-Added = {2018-02-14 12:11:00 PM +0000},
	Date-Modified = {2018-02-14 12:11:00 PM +0000},
	Day = {01},
	Doi = {10.1023/A:1007515423169},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Jul},
	Number = {1},
	Pages = {105--139},
	Title = {An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants},
	Volume = {36},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1007515423169},
	Bdsk-Url-2 = {http://dx.doi.org/10.1023/A:1007515423169}}

@article{buja2006,
	Author = {Andreas Buja and Werner Stuetzle},
	Date-Added = {2018-02-06 2:18:04 PM +0000},
	Date-Modified = {2018-02-06 2:19:11 PM +0000},
	Journal = {Statistica Sinica},
	Month = {April 2006},
	Number = {2},
	Title = {Observations on Bagging},
	Volume = {16},
	Year = {2006}}

@article{friedman2007,
	Author = {Friedman, Jerome H and Hall, Peter},
	Journal = {Journal of Statistical Planning and Inference},
	Number = {3},
	Pages = {669--683},
	Publisher = {Elsevier},
	Title = {On bagging and nonlinear estimation},
	Volume = {137},
	Year = {2007}}

@article{Hyafil1976,
	Author = {Hyafil, Laurent and Rivest, Ronald L},
	Journal = {Information Processing Letters},
	Number = {1},
	Pages = {15--17},
	Publisher = {Elsevier},
	Title = {Constructing optimal binary decision trees is NP-complete},
	Volume = {5},
	Year = {1976}}

@article{friedman1991,
	Author = {Friedman, Jerome H},
	Journal = {The Annals of Statistics},
	Pages = {1--67},
    Volume = {19},
    Number = {1},
	Publisher = {JSTOR},
	Title = {Multivariate adaptive regression splines},
	Year = {1991}}

@book{Gareth2013,
	Address = {New York, NY},
	Author = {Gareth, James},
	Isbn = {9781461471387},
	Keywords = {Mathematical statistics; Statistics; g Mathematics; Econometrics; g Statistics; Probabilities; Sampling (Statistics)},
	Language = {eng},
	Publisher = {Springer},
	Series = {Springer Texts in Statistics; 103},
	Title = {An Introduction to Statistical Learning with Applications in R},
	Year = {2013}}

@book{Breiman1984,
	Author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J and Olshen, Richard A},
	Publisher = {Wadsworth, Belmont, CA},
	Title = {Classification and regression trees},
	Year = {1984}}

@article{Buhlmann2002,
	Abstract = {Bagging is one of the most effective computationally intensive procedures to improve on unstable estimators or classifiers, useful especially for high dimensional data set problems. Here we formalize the notion of instability and derive theoretical results to analyze the variance reduction effect of bagging (or variants thereof) in mainly hard decision problems, which include estimation after testing in regression and decision trees for regression functions and classifiers. Hard decisions create instability, and bagging is shown to smooth such hard decisions, yielding smaller variance and mean squared error. With theoretical explanations, we motivate subagging based on subsampling as an alternative aggregation scheme. It is computationally cheaper but still shows approximately the same accuracy as bagging. Moreover, our theory reveals improvements in first order and in line with simulation studies. In particular, we obtain an asymptotic limiting distribution at the cube-root rate for the split point when fitting piecewise constant functions. Denoting sample size by n, it follows that in a cylindric neighborhood of diameter n-1/3 of the theoretically optimal split point, the variance and mean squared error reduction of subagging can be characterized analytically. Because of the slow rate, our reasoning also provides an explanation on the global scale for the whole covariate space in a decision tree with finitely many splits.},
	Author = {Peter B{\"u}hlmann and Bin Yu},
	Date-Modified = {2018-01-04 11:56:49 AM +0000},
	Issn = {00905364},
	Journal = {The Annals of Statistics},
	Number = {4},
	Pages = {927-961},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Analyzing Bagging},
	Volume = {30},
	Year = {2002},
	Bdsk-Url-1 = {http://www.jstor.org/stable/1558692}}

@article{Breiman1996,
	Abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	Author = {Breiman, Leo},
	Day = {01},
	Doi = {10.1007/BF00058655},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Aug},
	Number = {2},
	Pages = {123--140},
	Title = {Bagging predictors},
	Volume = {24},
	Year = {1996},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00058655},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/BF00058655}}


@article{politis1994,
	Author = {Politis, Dimitris N. and Romano, Joseph P.},
	Date-Modified = {2018-01-04 1:24:59 PM +0000},
	Doi = {10.1214/aos/1176325770},
	Fjournal = {The Annals of Statistics},
	Journal = {The Annals of Statistics},
	Month = {12},
	Number = {4},
	Pages = {2031--2050},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Large Sample Confidence Regions Based on Subsamples under Minimal Assumptions},
	Volume = {22},
	Year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1176325770},
	Bdsk-Url-2 = {http://dx.doi.org/10.1214/aos/1176325770}}

@article{scikit-learn2011,
	Author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	Journal = {Journal of Machine Learning Research},
	Pages = {2825--2830},
	Title = {Scikit-learn: Machine Learning in {P}ython},
	Volume = {12},
	Year = {2011}}

@book{EoSL,
	Address = {New York},
	Author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H},
	Bibsource = {Primo, http://link.bib.uni-mannheim.de/primo/dedupmrg133679088},
	Edition = {2},
	Isbn = {9780387848570},
	Keywords = {Statistik, Mathematische Statistik, Statistische Mathematik, Statistische Methode, Statistisches Verfahren, Statistiken / Maschinelles Lernen / Algorithmisches Lernen / Lernen / Automated learning / Machine learning, Statistik / Maschinelles Lernen},
	Language = {eng},
	Publisher = {Springer},
	Series = {Springer series in statistics},
	Title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
	Year = {2009}}

@article{breiman1996B,
	Author = {Breiman, Leo},
	Journal = {The Annals of Statistics},
	Number = {6},
	Pages = {2350--2383},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Heuristics of Instability and Stabilization in Model Selection},
	Volume = {24},
	Year = {1996}}

@article{Harrison1978,
	Author = {Harrison Jr, David and Rubinfeld, Daniel L},
	Journal = {Journal of Environmental Economics and Management},
	Number = {1},
	Pages = {81--102},
	Publisher = {Elsevier},
	Title = {Hedonic housing prices and the demand for clean air},
	Volume = {5},
	Year = {1978}}

@article{breiman1998,
	Author = {Breiman, Leo},
	Doi = {10.1214/aos/1024691079},
	Fjournal = {The Annals of Statistics},
	Journal = {The Annals of Statistics},
	Month = {06},
	Number = {3},
	Pages = {801--849},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Arcing Classifier (with discussion)},
	Volume = {26},
	Year = {1998},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1024691079},
	Bdsk-Url-2 = {http://dx.doi.org/10.1214/aos/1024691079}}

@book{Gentle2012,
	Author = {Gentle, James E and H{\"a}rdle, Wolfgang Karl and Mori, Yuichi},
	Publisher = {Springer Science \& Business Media},
	Title = {Handbook of computational statistics: concepts and methods},
	Year = {2012}}

@book{Lior2014,
	Author = {Lior, Rokach and others},
	Publisher = {World scientific},
	Title = {Data mining with decision trees: theory and applications},
	Volume = {81},
	Year = {2014}}

@book{grimmett,
	Author = {Grimmett, G. and Stirzaker, D.},
	Date-Modified = {2018-02-25 2:24:45 PM +0000},
	Isbn = {9780198572220},
	Lccn = {00103406},
	Publisher = {OUP Oxford},
	Title = {Probability and Random Processes},
	Year = {2001},
}

@book{serfling,
author = {Serfling, Robert J.},
year = {1980},
month = {02},
pages = {},
title = {Approximation Theorems of Mathematical Statistics},
Publisher = {Wiley},
Address = {New York, NY}
}
