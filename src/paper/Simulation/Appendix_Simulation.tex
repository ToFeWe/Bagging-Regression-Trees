\subsection{The Implementation of the Bagging Algorithm}\label{sec:Sim_Impl_App}
According to Definition \ref{bagging}, the Bagging Algorithm as described by \cite{Breiman1996} has been implemented in \texttt{Python} using \texttt{NumPy}. Our algorithm uses Regression Trees that are \hfill built \hfill by \hfill the \hfill \texttt{sklearn} \hfill package \hfill by \hfill \cite{scikit-learn2011}. \hfill Specifically, \hfill we \hfill use \hfill the \hfill module

\texttt{sklearn.trees.DecisionTreeRegressor}, which is a version of CART.\footnote{For further information we refer you to \url{http://scikit-learn.org/stable/modules/tree.html\#tree-algorithms-id3-c4-5-c5-0-and-cart }.} Building the optimal Regression Tree is computationally infeasible as shown by \cite{Hyafil1976}. To bypass this problem partially, \texttt{sklearn} uses a slightly optimized version of the CART-Algorithm.\footnote{\texttt{Sklearn} selects a single Regression Tree by training multiple Trees, where the features and corresponding observations are randomly sampled with replacement. Note that this random selection procedure was not used in the original CART-Algorithm introduced by \cite{Breiman1984}. The \texttt{sklearn} algorithm stays close to CART in the other aspects and is therefore a reasonable choice for the simulation studies. See \url{http://scikit-learn.org/stable/modules/tree.html\#decision-trees} for further information on this.}

The Bagging algorithm itself can be described as follows:\footnote{This pseudo code was inspired by \url{http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/ensembles/RandomForests.pdf}.}

\begin{algorithm}
    \caption{Bagging Algorithm with Regression Trees}
    \label{fit-bagging}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
      \Require Training Sample $L = \{y_{i},\mathbf{x_{i}} \}_{i=1}^{n}$ and $B$ number of bootstrap iterations

        \Function{BaggingAlgorithm}{$L,B$}
            \State{$T \gets  \emptyset$} \Comment{T is the container for all following Trees}
            \For{$j$ in 1 to $B$}
                \State $L^{j*} \gets $ Draw bootstrap sample from $L$
                \State $t^j \gets$ \Call{RegressionTree}{$L^{j*}$}
                \State $T \gets T \cup t^j$
            \EndFor
            \State \textbf{return} $T$
        \EndFunction

        \Function{RegressionTree}{$L^*$}
        \State $t \gets$ Train Regression Tree on $L^*$
        \State \textbf{return} $t$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Once we have applied the Bagging Algorithm with Regression Trees to some training set $L$, we can make a prediction on a test set $G$ in the following way:

\begin{algorithm}
  \caption{Prediction for Bagging Algorithm with Trees}
  \label{predict-bagging}
  \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Require Test Sample $G = \{y_{i},\mathbf{x_{i}} \}_{i=1}^{m}$ and a trained Bagging instance $T$ as defined by Algorithm \ref{fit-bagging} consisting of $B$ different Regression Trees
    \Function{PredictBagging}{$G,T$}
      \For{$j$ in 1 to $B$}
      \State $p^j \gets$ Prediction with $t^j \in T$ for $G$
      \State $P \gets P \cup p^j$
      \EndFor
      \State $A \gets$ For each observation $j \in G$ average over all $B$ predictions in $P$
      \State \textbf{return} $A$
    \EndFunction

  \end{algorithmic}
\end{algorithm}

The array $A$ then consists of the averaged predictions, based on the trained Bagging instance $T$, for each observation in the training set $G$ and is therefore of shape $(m,1)$. Hence, we make for each observation in the training set $G$, $B$-many predictions and average over those outcomes to obtain the array of final predictions.

\subsection{Further Simulation Results}
We report here the results for the indicator model M3 as described by \cite{buhlmann2003bagging}.
As it was not part of the original project paper, we do not report the functional form here but we
refer you to \cite{buhlmann2003bagging}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{../../out/figures/main_simulation/plot_simulation_tree_depth_appendix.pdf}
\caption[]{The M3 Indicator Model: The effect of varying the complexity of the Regression Tree for the unbagged and bagged Tree. The minimal size for each terminal node is displayed on the $x$-axis. A higher minimal size for each terminal node implies a less complex Tree.}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[scale=0.7]{../../out/figures/main_simulation/plot_simulation_subagging_appendix.pdf}
\caption[]{The M3 Indicator Model: The effectiveness of Subagging and Bagging in comparison to the unbagged Regression Tree. The subsample ratio \textit{a} for Subagging is on the $x$-axis.}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[scale=0.7]{../../out/figures/main_simulation/plot_simulation_convergence_appendix.pdf}
\caption[]{The M3 Indicator Model: The effect of varying the number of bootstrap iterations \textit{B} in the case of Bagging. The effect of varying the number of bootstrap iterations \textit{B} in the case of Bagging. The minimal number of bootstrap iterations is chosen to 2. The light blue line indicates the MSPE for \textit{B}=500}
\end{figure}
