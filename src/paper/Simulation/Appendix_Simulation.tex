\subsection{The Implementation of the Bagging Algorithm}\label{sec:Sim_Impl_App}
According to Definition \ref{bagging}, the Bagging Algorithm as described by \cite{Breiman1996} has been implemented in \texttt{Python} using \texttt{NumPy}. Our algorithm uses Regression Trees that are \hfill built \hfill by \hfill the \hfill \texttt{sklearn} \hfill package \hfill by \hfill \cite{scikit-learn2011}. \hfill Specifically, \hfill we \hfill use \hfill the \hfill module

\texttt{sklearn.trees.DecisionTreeRegressor}, which is a version of CART.\footnote{For further information we refer you to \url{http://scikit-learn.org/stable/modules/tree.html\#tree-algorithms-id3-c4-5-c5-0-and-cart }.} Building the optimal Regression Tree is computationally infeasible as shown by \cite{Hyafil1976}. To bypass this problem partially, \texttt{sklearn} uses a slightly optimized version of the CART-Algorithm.\footnote{\texttt{Sklearn} selects a single Regression Tree by training multiple Trees, where the features and corresponding observations are randomly sampled with replacement. Note that this random selection procedure was not used in the original CART-Algorithm introduced by \cite{Breiman1984}. The \texttt{sklearn} algorithm stays close to CART in the other aspects and is therefore a reasonable choice for the simulation studies. See \url{http://scikit-learn.org/stable/modules/tree.html\#decision-trees} for further information on this.}

The Bagging algorithm itself can be described as follows:\footnote{This pseudo code was inspired by \url{http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/ensembles/RandomForests.pdf}.}

\begin{algorithm}
    \caption{Bagging Algorithm with Regression Trees}
    \label{fit-bagging}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
      \Require Training Sample $L = \{y_{i},\mathbf{x_{i}} \}_{i=1}^{n}$ and $B$ number of bootstrap iterations

        \Function{BaggingAlgorithm}{$L,B$}
            \State{$T \gets  \emptyset$} \Comment{T is the container for all following Trees}
            \For{$j$ in 1 to $B$}
                \State $L^{j*} \gets $ Draw bootstrap sample from $L$
                \State $t^j \gets$ \Call{RegressionTree}{$L^{j*}$}
                \State $T \gets T \cup t^j$
            \EndFor
            \State \textbf{return} $T$
        \EndFunction

        \Function{RegressionTree}{$L^*$}
        \State $t \gets$ Train Regression Tree on $L^*$
        \State \textbf{return} $t$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Once we have applied the Bagging Algorithm with Regression Trees to some training set $L$, we can make a prediction on a test set $G$ in the following way:

\begin{algorithm}
  \caption{Prediction for Bagging Algorithm with Trees}
  \label{predict-bagging}
  \begin{algorithmic}[1] % The number tells where the line numbering should start
    \Require Test Sample $G = \{y_{i},\mathbf{x_{i}} \}_{i=1}^{m}$ and a trained Bagging instance $T$ as defined by Algorithm \ref{fit-bagging} consisting of $B$ different Regression Trees
    \Function{PredictBagging}{$G,T$}
      \For{$j$ in 1 to $B$}
      \State $p^j \gets$ Prediction with $t^j \in T$ for $G$
      \State $P \gets P \cup p^j$
      \EndFor
      \State $A \gets$ For each observation $j \in G$ average over all $B$ predictions in $P$
      \State \textbf{return} $A$
    \EndFunction

  \end{algorithmic}
\end{algorithm}

The array $A$ then consists of the averaged predictions, based on the trained Bagging instance $T$, for each observation in the training set $G$ and is therefore of shape $(m,1)$. Hence, we make for each observation in the training set $G$, $B$-many predictions and average over those outcomes to obtain the array of final predictions.

\begin{comment}
\subsection{Tree Depth and Bagging}\label{sec:Sim_TreeDepth}
In the simulation employed in Section \ref{sec:Simulation}, we always choose to grow the Trees fully. In this part we want to give some insight why this is optimal, when Bagging is applied to Regression Trees. \newline
As discussed in Section \ref{sec:BiasVariance}, Regression Trees tend to overfit the data. Overfitting is specifically a problem, when we grow a Tree to its maximal size. The terminal leafs will consist of only a single observation, making it thereby likely that we fit the noise in the data. Thus usually when fitting single Trees to the data it is common to restrict the size of the Tree to reduce overfitting and thereby reduce the variance. \newline
Interestingly, it turns out that it is \textit{not} the optimal choice in terms of MSPE to restrict the Tree size if we apply Bagging to Regression Trees as we can see in Figure \ref{fig:leafsize}.
\begin{figure}[H]
\centering

\includegraphics[scale=0.6]{../../out/figures/main_simulation/plot_simulation_tree_depth.pdf}
\caption[The effect of varying the complexity of the Regression Tree for the normal and bagged Tree.]{The effect of varying the complexity of the Regression Tree for the normal and bagged Tree. The minimal size for each leaf is displayed on the $x$-axis. A higher minimal size for each leaf implies a less complex Tree.}
\label{fig:leafsize}
\end{figure}

In Figure \ref{fig:leafsize} we plot the MSPE, the variance and the squared-bias for bagged and unbagged Trees against the Tree depth. We control the size of each Tree by restricting the minimal amount of observations in each terminal node. A larger minimal size for each terminal node implies a smaller and thus less complex Tree.
We report the results for the unbagged Regression Tree and Bagging applied to Regression Trees as in Section \ref{sub:bagging_table} for both regression functions.
\newline
The MSPE for Trees is minimized at a smaller Tree size as for Bagging. Whereas for Bagging we obtain the best result by growing the Tree to the maximal length, the optimum for normal Regression Trees can be achieved by restricting the minimal size for each terminal node to around 20 observations. Lower values result in a higher MSPE for Trees. While we decrease the squared-bias by increasing the size of the Tree, we overfit the data and have a predictor with a high variance. \newline
Bagging on the other hand works best if there are no restrictions on the Tree size. As discussed in Section \ref{bag} and \ref{sec:bagregtree}, Bagging reduces the variance of the predictor while leaving the squared-bias almost unaffected. As Regression Trees have the lowest bias for maximal grown Trees, it is therefore optimal not to restrict the size of the Trees, when using Bagging.
\end{comment}
