@book{Gareth2013,
	Address = {New York, NY},
	Author = {Gareth, James},
	Isbn = {9781461471387},
	Publisher = {Springer},
	Series = {Springer Texts in Statistics; 103},
	Title = {An Introduction to Statistical Learning with Applications in R},
	Year = {2013}}

@book{cart84,
  author        = {L. Breiman and J. Friedman and R. Olshen and C. Stone},
  title         = {{Classification and Regression Trees}},
  publisher     = {Wadsworth and Brooks},
  address       = {Monterey, CA},
  year          = {1984}
}

@article{Buhlmann2002,
	Abstract = {Bagging is one of the most effective computationally intensive procedures to improve on unstable estimators or classifiers, useful especially for high dimensional data set problems. Here we formalize the notion of instability and derive theoretical results to analyze the variance reduction effect of bagging (or variants thereof) in mainly hard decision problems, which include estimation after testing in regression and decision trees for regression functions and classifiers. Hard decisions create instability, and bagging is shown to smooth such hard decisions, yielding smaller variance and mean squared error. With theoretical explanations, we motivate subagging based on subsampling as an alternative aggregation scheme. It is computationally cheaper but still shows approximately the same accuracy as bagging. Moreover, our theory reveals improvements in first order and in line with simulation studies. In particular, we obtain an asymptotic limiting distribution at the cube-root rate for the split point when fitting piecewise constant functions. Denoting sample size by n, it follows that in a cylindric neighborhood of diameter n-1/3 of the theoretically optimal split point, the variance and mean squared error reduction of subagging can be characterized analytically. Because of the slow rate, our reasoning also provides an explanation on the global scale for the whole covariate space in a decision tree with finitely many splits.},
	Author = {Peter B{\"u}hlmann and Bin Yu},
	Date-Modified = {2018-01-04 11:56:49 AM +0000},
	Issn = {00905364},
	Journal = {The Annals of Statistics},
	Number = {4},
	Pages = {927-961},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Analyzing Bagging},
	Url = {http://www.jstor.org/stable/1558692},
	Volume = {30},
	Year = {2002},
	Bdsk-Url-1 = {http://www.jstor.org/stable/1558692}}

@article{Breiman1996,
	Abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	Author = {Breiman, Leo},
	Day = {01},
	Doi = {10.1007/BF00058655},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Aug},
	Number = {2},
	Pages = {123--140},
	Title = {Bagging predictors},
	Url = {https://doi.org/10.1007/BF00058655},
	Volume = {24},
	Year = {1996},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00058655},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/BF00058655}}

@article{ChettyHendrenKlineSaez2014,
	Author = {Chetty, Raj and Hendren, Nathaniel and Kline, Patrick and Saez, Emmanuel},
	Doi = {10.1093/qje/qju022},
	Eprint = {/oup/backfile/content_public/journal/qje/129/4/10.1093_qje_qju022/4/qju022.pdf},
	Journal = {The Quarterly Journal of Economics},
	Number = {4},
	Pages = {1553-1623},
	Title = {Where is the land of Opportunity? The Geography of Intergenerational Mobility in the United States *},
	Url = {+ http://dx.doi.org/10.1093/qje/qju022},
	Volume = {129},
	Year = {2014},
	Bdsk-Url-1 = {+%20http://dx.doi.org/10.1093/qje/qju022},
	Bdsk-Url-2 = {http://dx.doi.org/10.1093/qje/qju022}}

@article{politis1994,
author = "Politis, Dimitris N. and Romano, Joseph P.",
doi = "10.1214/aos/1176325770",
fjournal = "The Annals of Statistics",
journal = "The Annals of Statistics",
month = "12",
number = "4",
pages = "2031--2050",
publisher = "The Institute of Mathematical Statistics",
title = "Large Sample Confidence Regions Based on Subsamples under Minimal Assumptions",
url = "https://doi.org/10.1214/aos/1176325770",
volume = "22",
year = "1994"
}

@book{EoSL,
	title = "The Elements of Statistical Learning : Data Mining, Inference, and Prediction",
	author = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H",
	publisher = "Springer",
	address = "New York, NY",
	year = "2009",
	edition = "2",
	isbn = "9780387848570",
	keywords = "Statistik, Mathematische Statistik, Statistische Mathematik, Statistische Methode, Statistisches Verfahren, Statistiken / Maschinelles Lernen / Algorithmisches Lernen / Lernen / Automated learning / Machine learning, Statistik / Maschinelles Lernen",
	series = "Springer series in statistics",
	language = "eng",
	bibsource = "Primo, http://link.bib.uni-mannheim.de/primo/dedupmrg133679088"
	}

@article{DietterichKong95,
  title={Machine learning bias, statistical bias, and statistical variance of decision tree algorithms},
  author={Dietterich, Thomas G and Kong, Eun Bae},
  year={1995},
  institution={Technical report, Department of Computer Science, Oregon State University}
}
